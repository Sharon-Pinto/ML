#Post Pruning
# Importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import tree
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

# take breast_cancer dataset available in sklearn'
#Load the dataset
X, y = load_breast_cancer(return_X_y=True)
# Split the dataset into training and test sets. By default, train_test_split splits the data into 75% training data and 25% test data.
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

# Create a Decision Tree classifier
clf = DecisionTreeClassifier(random_state=0)

# Fit the Decision Tree classifier to the training data.
clf.fit(X_train, y_train)
# Predict the target labels for the training data using the trained classifier.
y_train_predicted = clf.predict(X_train)

# Predict the target labels for the test data using the trained classifier.
y_test_predicted = clf.predict(X_test)

# Calculate and print the accuracy score for the training data predictions compared to the actual training labels.
accuracy_score(y_train, y_train_predicted)
 
# Calculate and print the accuracy score for the test data predictions compared to the actual test labels.
accuracy_score(y_test, y_test_predicted)
 
# Create a figure with a specified size for the Decision Tree visualization.
plt.figure(figsize=(16, 8))
 
# Use the tree.plot_tree function to visualize the Decision Tree classifier 'clf'.
tree.plot_tree(clf)
# Display the Decision Tree plot.
plt.show()
 
# Now we do post pruning operation.
# We use cost_complexity_pruning technique to prune the branches of decision tree.

#we use cost_complexity_pruning technique to prune the branches of decision tree.
path=clf.cost_complexity_pruning_path(X_train,y_train)
#path variable gives two things ccp_alphas and impurities
ccp_alphas,impurities=path.ccp_alphas,path.impurities
print("ccp alpha wil give list of values :",ccp_alphas)
print("***********************************************************")
print("Impurities in Decision Tree :",impurities)

clfs = []  # will store all the models here
# Iterate through different values of 'ccp_alpha' in 'ccp_alphas'.
for ccp_alpha in ccp_alphas:
    # Create a Decision Tree classifier with a specified random state and the current 'ccp_alpha'.
    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)
     # Fit the Decision Tree classifier to the training data.
    clf.fit(X_train, y_train)
    # Append the trained classifier to the 'clfs' list.
    clfs.append(clf)
    # Print the information about the last node in the Decis
#Visualizing the accuracy score for train and test set.
train_scores = [clf.score(X_train, y_train) for clf in clfs]
test_scores = [clf.score(X_test, y_test) for clf in clfs]
fig, ax = plt.subplots()
ax.set_xlabel("alpha")
ax.set_ylabel("accuracy")
ax.set_title("Accuracy vs alpha for training and testing sets")
ax.plot(ccp_alphas, train_scores, marker='o', label="train",drawstyle="steps-post")
ax.plot(ccp_alphas, test_scores, marker='o', label="test",drawstyle="steps-post")
ax.legend()
plt.show()
 
# If we folow bias and variance tradeoff we will choose that point which will have low bias(low training error) and low variance(low test error).
# Here we get that point at a value of alpha=0.02.
clf=DecisionTreeClassifier(random_state=0,ccp_alpha=0.02)
clf.fit(X_train,y_train)
plt.figure(figsize=(12,8))
tree.plot_tree(clf,rounded=True,filled=True)
plt.show()
 
accuracy_score(y_test,clf.predict(X_test))

========PrePruning================================================================
# Get the optimal parameters by hyperparameter tuning
# GridSearchCV
grid_param={"criterion":["gini","entropy"],
             "splitter":["best","random"],
             "max_depth":range(2,50,1),
             "min_samples_leaf":range(1,15,1),
             "min_samples_split":range(2,20,1)
            }
grid_search=GridSearchCV(estimator=clf,param_grid=grid_param,cv=5,n_jobs=-1)
grid_search.fit(X_train,y_train)
 
print(grid_search.best_params_)
# Pre-Pruning Operation
clf=DecisionTreeClassifier(criterion= 'entropy',max_depth= 8,min_samples_leaf= 3,min_samples_split= 2,splitter= 'random')
clf.fit(X_train,y_train)
plt.figure(figsize=(20,12))
tree.plot_tree(clf,rounded=True,filled=True)
plt.show()
 
y_predicted=clf.predict(X_test)
accuracy_score(y_test,y_predicted)
